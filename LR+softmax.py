import pandas as pd
import numpy as np
import torch
import random

from torchinfo import summary

from models.LinearInferencer import LinearPredictorTorch
from DataPipeline.Dataloader import PortfolioDataset
from torch.utils.data import DataLoader
from DataPipeline.DataBuilder import build_dataset
from torch import nn
from torch.optim import Adam
pd.options.display.float_format = '{:.6f}'.format
np.set_printoptions(precision=6, suppress=True)

tickers = ["EEM","EFA","JPXN","SPY","XLK",'VTI','AGG','DBC']

# Random Seed
seed = 123
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)

class FNNSoftmaxAllocator(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_assets):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_assets)
        )

    def forward(self, preds):  # preds: (B, N)
        scores = self.net(preds)     # (B, N)
        weights = torch.softmax(scores, dim=1)
        return weights
    
def spo_plus_loss(pred_y, true_y, allocator):
    pred_weights = allocator(pred_y)        # Åµ = g(Å·)
    oracle_weights = allocator(true_y)      # w* â‰ˆ g(y)

    regret = torch.sum((oracle_weights - pred_weights) * true_y, dim=1)
    return regret.mean()

class LinearPredictorTorch(nn.Module):
    def __init__(self, input_dim, num_assets):
        super().__init__()
        self.linear = nn.Linear(input_dim, num_assets)  # ä¸€æ¬¡æ€§é¢„æµ‹å…¨éƒ¨ ETF

    def forward(self, x):
        """
        x: shape = (batch_size, num_assets, input_dim)
        è¾“å‡º: shape = (batch_size, num_assets)
        """
        x = x.view(x.size(0), -1)  # (batch, 8, 7) â†’ (batch, 56)
        return self.linear(x)


# æ¨¡å‹è¶…å‚æ•°
input_dim = 7         # æ¯ä¸ªèµ„äº§çš„ç‰¹å¾æ•°
num_assets = 8        # ETF æ•°é‡
hidden_dim = 32       # allocator éšå±‚å®½åº¦
epochs = 30           # è®­ç»ƒè½®æ•°
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# æ¨¡å‹å®ä¾‹åŒ–
predictor = LinearPredictorTorch(input_dim * num_assets, num_assets).to(device)
allocator = FNNSoftmaxAllocator(num_assets, hidden_dim, num_assets).to(device)
print(summary(allocator, input_size=(64, num_assets)))
# ä¼˜åŒ–å™¨ï¼ˆè”åˆè®­ç»ƒ predictor å’Œ allocatorï¼‰
optimizer = Adam(list(predictor.parameters()) + list(allocator.parameters()), lr=1e-3)
features_df, labels_df = build_dataset(
    tickers=["SPY", "VTI", "EFA", "EEM", "XLK", "JPXN", "AGG", "DBC"],
    start_date="2023-01-01",
    end_date="2023-12-31")

oracle_df = pd.read_csv("data/DailyOracle/oracle_weights_with_fee.csv", index_col=0)
features_df.index = pd.to_datetime(features_df.index).normalize()
oracle_df.index = pd.to_datetime(oracle_df.index).normalize()
oracle_df = oracle_df.loc[features_df.index]
if len(features_df) != len(oracle_df):
    raise ValueError("features_df å’Œ oracle_df è¡Œæ•°ä¸ä¸€è‡´ï¼Œä¸èƒ½å¯¹é½ï¼")

labels_df = oracle_df.copy()
print(labels_df)
# åˆ›å»º dataset
dataset = PortfolioDataset(features_df, labels_df, num_assets=8)

# åˆ›å»º dataloader
train_loader = DataLoader(dataset, batch_size=63, shuffle=True)
# è®­ç»ƒå¾ªç¯
for epoch in range(epochs):
    total_loss = 0
    for x, y in train_loader:
        x = x.to(device)  # shape: (B, N, F)
        y = y.to(device)  # shape: (B, N)
        # å‰å‘ä¼ æ’­
        pred_y = predictor(x)  # shape: (B, N)
        loss = spo_plus_loss(pred_y, y, allocator)
    
        # åå‘ä¼ æ’­ä¸ä¼˜åŒ–
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item() * x.size(0)

    avg_loss = total_loss / len(train_loader.dataset)
    print(f"Epoch {epoch+1:02d} - Loss: {avg_loss:.6f}")

# è®¾å®šç›®æ ‡æ¨æ–­æœˆä»½
infer_start = "2024-01-01"
infer_end = "2024-01-31"

# ğŸ” å•ç‹¬æ„å»ºç›®æ ‡æœˆä»½çš„ç‰¹å¾æ•°æ®ï¼ˆç¡®ä¿ä¸å¸¦è®­ç»ƒæ•°æ®ï¼‰
features_future, labels_future = build_dataset(
    tickers=tickers,
    start_date=infer_start,
    end_date=infer_end
)

# ä¿è¯ç´¢å¼•æ˜¯æ ‡å‡†æ—¥æœŸæ ¼å¼
features_future.index = pd.to_datetime(features_future.index).normalize()
labels_future.index = pd.to_datetime(labels_future.index).normalize()

# æ„é€ æ¨æ–­ç”¨ Datasetï¼ˆåªç”¨ xï¼‰
inference_dataset = PortfolioDataset(features_future, labels_future, num_assets=8)

# è½¬ä¸ºå¼ é‡ (B, 8, 7)
x_tensor = torch.stack([inference_dataset[i][0] for i in range(len(inference_dataset))]).to(device)

# ğŸ” æ¨æ–­é˜¶æ®µ
predictor.eval()
allocator.eval()
with torch.no_grad():
    pred_y = predictor(x_tensor)        # shape: (B, 8)
    pred_weights = allocator(pred_y)   # shape: (B, 8)

# èšåˆç»“æœ
w_next_month = pred_weights.mean(dim=0).cpu().numpy()

# æ‰“å°ç»„åˆ
print(f"âœ… æ¨æ–­å¾—åˆ°çš„ 2024-01 ç»„åˆæ¯”ç‡ï¼š")
for ticker, weight in zip(tickers, w_next_month):
    print(f"{ticker}: {weight:.4f}")






